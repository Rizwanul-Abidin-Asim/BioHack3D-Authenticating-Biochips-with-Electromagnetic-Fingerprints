{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rizwanul-Abidin-Asim/BioHack3D-Authenticating-Biochips-with-Electromagnetic-Fingerprints/blob/main/BioHack3D_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvLi4sRAORZ5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "0b0d6262-d40d-4513-b0b0-94b31a3f554c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'Augmentor'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e50c3a85fa14>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mAugmentor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrembg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mremove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Augmentor'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import Augmentor\n",
        "import numpy as np\n",
        "from rembg import remove\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, auc, roc_curve\n",
        "from keras.layers import Conv2D, GlobalAveragePooling2D, Flatten, Dense, Dropout\n",
        "from keras.applications.xception import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from keras.applications import Xception, InceptionV3, DenseNet121,VGG19,VGG16\n",
        "from keras.regularizers import l1\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rcParams\n",
        "from skimage.util import random_noise\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "XwZ2xb2GAK7i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "d49f2a02-cbf6-4f95-8400-a03488a1ef92"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVU1jhsfOtiL",
        "outputId": "05815b10-a015-44e4-b24c-4598acbc0cdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "780\n",
            "402\n"
          ]
        }
      ],
      "source": [
        "# Set the path to your dataset directory\n",
        "dataset_dir1 = r'/content/drive/MyDrive/BioHACK3D 2024_For Participants/Dataset_BioHACK3D_2024/Authentic'\n",
        "dataset_dir2 = r'/content/drive/MyDrive/BioHACK3D 2024_For Participants/Dataset_BioHACK3D_2024/Inauthentic'\n",
        "processed_data_dir = r'/content/drive/MyDrive/Processed'\n",
        "\n",
        "# Set the image dimensions\n",
        "img_width, img_height = 224, 224\n",
        "\n",
        "# Load the image filenames and labels\n",
        "original_files = glob.glob(os.path.join(dataset_dir1, '**','*.jpg'),recursive=True)\n",
        "print(len(original_files))\n",
        "duplicate_files = glob.glob(os.path.join(dataset_dir2, '**','*.jpg'),recursive=True)\n",
        "print(len(duplicate_files))\n",
        "\n",
        "# Preprocessing Functions\n",
        "def preprocess_image(image_path):\n",
        "    with open(image_path, 'rb') as i:\n",
        "        image_data = i.read()\n",
        "\n",
        "    # Load the image from the result of rembg\n",
        "    nparr = np.frombuffer(image_data, np.uint8)\n",
        "    image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "\n",
        "    # Resize to 224x224\n",
        "    image = cv2.resize(image, (224, 224))\n",
        "\n",
        "    # Normalize to [0, 1]\n",
        "    image = image / 255.0\n",
        "\n",
        "    return image\n",
        "\n",
        "# Process and Save Images\n",
        "def process_and_save_images(input_dir, output_dir, label):\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    i=0\n",
        "    for file_name in input_dir:\n",
        "        processed_image = preprocess_image(file_name)\n",
        "        save_path = os.path.join(output_dir, label + \"_\" + str(i)+\".jpg\")\n",
        "        cv2.imwrite(save_path, (processed_image * 255).astype(np.uint8))\n",
        "        #print(save_path)\n",
        "        i+=1\n",
        "process_and_save_images(original_files, processed_data_dir + \"/authentic\", \"authentic\")\n",
        "process_and_save_images(duplicate_files, processed_data_dir + \"/non_authentic\", \"non_authentic\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbwAJCYEbvxp"
      },
      "outputs": [],
      "source": [
        "processed_data_dir = r'/content/drive/MyDrive/Processed'  # Adjust the path\n",
        "augmented_data_dir = r'/content/drive/MyDrive/Augmented'   # Where you want to save augmented images\n",
        "\n",
        "# Create an output directory for augmented images\n",
        "if not os.path.exists(augmented_data_dir):\n",
        "    os.makedirs(augmented_data_dir)\n",
        "\n",
        "# Create data augmentations for each class\n",
        "for class_name in ['authentic', 'non_authentic']:\n",
        "    # Create a pipeline for the current class\n",
        "    p = Augmentor.Pipeline(os.path.join(processed_data_dir, class_name),\n",
        "                           output_directory=os.path.join(augmented_data_dir, class_name))\n",
        "\n",
        "    # Define augmentation operations----you can write code for different augmentation operations\n",
        "    # Augmentation operations\n",
        "    p.flip_top_bottom(probability=0.5)\n",
        "    p.flip_left_right(probability=0.5)  # Adds a left-right flip\n",
        "    p.rotate(probability=0.7, max_left_rotation=25, max_right_rotation=25)\n",
        "    p.zoom_random(probability=0.5, percentage_area=0.9)  # Slight zoom, retaining 90% of the original area\n",
        "\n",
        "\n",
        "    # Sample augmented images (you can adjust this number as needed)\n",
        "    p.sample(50*100)  # Number of augmented images to create per original image\n",
        "\n",
        "# Data Generators\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "\n",
        "# Make sure to point to the new augmented data directory\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    augmented_data_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    augmented_data_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='validation'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZpoJm5ub-ly"
      },
      "outputs": [],
      "source": [
        "images, labels = next(train_generator)\n",
        "print(\"Labels in this batch:\", np.sum(labels==0))\n",
        "plt.figure(figsize=(3, 3 ))\n",
        "plt.imshow(images[1], cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJKs0ZGMh1uM"
      },
      "outputs": [],
      "source": [
        "#USE THIS CELL\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l1\n",
        "\n",
        "# Set up MobileNetV2 base model\n",
        "mobilenet_base = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "mobilenet_base.trainable = False  # Freeze all layers initially\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential([\n",
        "    mobilenet_base,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(128, activation='relu', kernel_regularizer=l1(0.0001)),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu', kernel_regularizer=l1(0.0001)),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train with Data Augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    brightness_range=[0.8, 1.2],\n",
        "    zoom_range=0.1,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    augmented_data_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    augmented_data_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Train the model with gradual fine-tuning\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=validation_generator,\n",
        "    epochs= 20 # Start with fewer epochs\n",
        ")\n",
        "\n",
        "# Unfreeze top layers for fine-tuning after initial training\n",
        "for layer in mobilenet_base.layers[-20:]:  # Adjust number of layers based on performance\n",
        "    layer.trainable = True\n",
        "\n",
        "# Recompile with a lower learning rate\n",
        "model.compile(optimizer=Adam(learning_rate=0.00001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Continue training with unfreezing layers\n",
        "history_fine_tune = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=validation_generator,\n",
        "    epochs= 10\n",
        ")\n",
        "with open(\"authenticity_classifier_history.pkl\", \"wb\") as file:\n",
        "    pickle.dump(history.history, file)\n",
        "\n",
        "# Save the model in .keras format\n",
        "model.save('authenticity_classifier.keras')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to evaluate precision, recall, F1 score, and confusion matrix\n",
        "def evaluate_model(model, generator):\n",
        "    # Predict probabilities\n",
        "    predictions = model.predict(generator, verbose=1)\n",
        "    # Convert probabilities to binary predictions\n",
        "    y_pred = (predictions > 0.5).astype(int).flatten()\n",
        "\n",
        "    # True labels from the generator\n",
        "    y_true = generator.classes\n",
        "\n",
        "    # Calculate metrics\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=generator.class_indices.keys())\n",
        "    disp.plot(cmap=plt.cm.Blues)\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "# Evaluate model on the validation data\n",
        "evaluate_model(model, validation_generator)\n"
      ],
      "metadata": {
        "id": "fyJZAkeHlLTa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}